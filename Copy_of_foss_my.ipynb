{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of foss my.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mithil2311/Conference/blob/master/Copy_of_foss_my.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr3R35TPDyuL",
        "colab_type": "code",
        "outputId": "3a98efdd-0ea7-4197-b629-2447601a5df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#dense layer\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "\n",
        "\n",
        "# # Parameters\n",
        "# learning_rate = 0.01\n",
        "# training_epochs = 25\n",
        "# batch_size = 100\n",
        "# display_step = 1\n",
        "\n",
        "# # tf Graph Input\n",
        "# x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
        "# y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
        "\n",
        "# # Set model weights\n",
        "# W = tf.Variable(tf.zeros([784, 10]))\n",
        "# b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# # Construct model\n",
        "# pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
        "\n",
        "# # Minimize error using cross entropy\n",
        "# cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
        "# # Gradient Descent\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# # Initialize the variables (i.e. assign their default value)\n",
        "# init = tf.global_variables_initializer()\n",
        "\n",
        "# # Start training\n",
        "# with tf.Session() as sess:\n",
        "\n",
        "#     # Run the initializer\n",
        "#     sess.run(init)\n",
        "\n",
        "#     # Training cycle\n",
        "#     for epoch in range(training_epochs):\n",
        "#         avg_cost = 0.\n",
        "#         total_batch = int(mnist.train.num_examples/batch_size)\n",
        "#         # Loop over all batches\n",
        "#         for i in range(total_batch):\n",
        "#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "#             # Run optimization op (backprop) and cost op (to get loss value)\n",
        "#             _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
        "#                                                           y: batch_ys})\n",
        "#             # Compute average loss\n",
        "#             avg_cost += c / total_batch\n",
        "#         # Display logs per epoch step\n",
        "#         if (epoch+1) % display_step == 0:\n",
        "#             print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "\n",
        "#     print(\"Optimization Finished!\")\n",
        "\n",
        "#     # Test model\n",
        "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "#     # Calculate accuracy\n",
        "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "#     print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "#     writer=tf.summary.FileWriter('output',sess.graph)\n",
        "\n",
        "\n",
        "\n",
        "image_size = 28\n",
        "labels_size = 10\n",
        "learning_rate = 0.05\n",
        "steps_number = 1000\n",
        "batch_size = 100\n",
        "\n",
        "# Define placeholders\n",
        "training_data = tf.placeholder(tf.float32, [None, image_size*image_size])\n",
        "labels = tf.placeholder(tf.float32, [None, labels_size])\n",
        "\n",
        "# Variables to be tuned\n",
        "W = tf.Variable(tf.truncated_normal([image_size*image_size, labels_size], stddev=0.1))\n",
        "b = tf.Variable(tf.constant(0.1, shape=[labels_size]))\n",
        "\n",
        "# Build the network (only output layer)\n",
        "output = tf.matmul(training_data, W) + b\n",
        "\n",
        "# Define the loss function\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output))\n",
        "\n",
        "# Training step\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "# Accuracy calculation\n",
        "correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# Run the training\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "for i in range(steps_number):\n",
        "  # Get the next batch\n",
        "  input_batch, labels_batch = mnist.train.next_batch(batch_size)\n",
        "  feed_dict = {training_data: input_batch, labels: labels_batch}\n",
        "\n",
        "  # Run the training step\n",
        "  train_step.run(feed_dict=feed_dict)\n",
        "\n",
        "  # Print the accuracy progress on the batch every 100 steps\n",
        "  if i%100 == 0:\n",
        "    train_accuracy = accuracy.eval(feed_dict=feed_dict)\n",
        "    print(\"Step %d, training batch accuracy %g %%\"%(i, train_accuracy*100))\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_accuracy = accuracy.eval(feed_dict={training_data: mnist.test.images, labels: mnist.test.labels})\n",
        "print(\"Test accuracy: %g %%\"%(test_accuracy*100))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, training batch accuracy 10 %\n",
            "Step 100, training batch accuracy 80 %\n",
            "Step 200, training batch accuracy 83 %\n",
            "Step 300, training batch accuracy 90 %\n",
            "Step 400, training batch accuracy 89 %\n",
            "Step 500, training batch accuracy 88 %\n",
            "Step 600, training batch accuracy 86 %\n",
            "Step 700, training batch accuracy 89 %\n",
            "Step 800, training batch accuracy 85 %\n",
            "Step 900, training batch accuracy 93 %\n",
            "Test accuracy: 89.82 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64KCM9-nUVKm",
        "colab_type": "code",
        "outputId": "9b3be86d-f0dc-4294-901b-c6b05d864efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 25\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# tf Graph Input\n",
        "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
        "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
        "\n",
        "# Set model weights\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Construct model\n",
        "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
        "\n",
        "# Minimize error using cross entropy\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
        "# Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
        "                                                          y: batch_ys})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if (epoch+1) % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    writer=tf.summary.FileWriter('output',sess.graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 cost= 1.183785420\n",
            "Epoch: 0002 cost= 0.665326614\n",
            "Epoch: 0003 cost= 0.552747847\n",
            "Epoch: 0004 cost= 0.498682152\n",
            "Epoch: 0005 cost= 0.465511117\n",
            "Epoch: 0006 cost= 0.442576258\n",
            "Epoch: 0007 cost= 0.425533246\n",
            "Epoch: 0008 cost= 0.412125704\n",
            "Epoch: 0009 cost= 0.401404024\n",
            "Epoch: 0010 cost= 0.392431483\n",
            "Epoch: 0011 cost= 0.384797055\n",
            "Epoch: 0012 cost= 0.378208999\n",
            "Epoch: 0013 cost= 0.372443758\n",
            "Epoch: 0014 cost= 0.367289742\n",
            "Epoch: 0015 cost= 0.362696686\n",
            "Epoch: 0016 cost= 0.358591311\n",
            "Epoch: 0017 cost= 0.354889909\n",
            "Epoch: 0018 cost= 0.351469850\n",
            "Epoch: 0019 cost= 0.348348706\n",
            "Epoch: 0020 cost= 0.345430499\n",
            "Epoch: 0021 cost= 0.342749796\n",
            "Epoch: 0022 cost= 0.340239885\n",
            "Epoch: 0023 cost= 0.337933695\n",
            "Epoch: 0024 cost= 0.335733768\n",
            "Epoch: 0025 cost= 0.333727062\n",
            "Optimization Finished!\n",
            "Accuracy: 0.914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWBvGm7TSiwS",
        "colab_type": "code",
        "outputId": "ab3d5dd4-22d0-4580-ed96-aec6ef387b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "#hidden layer\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "def train_network(training_data, labels, output, keep_prob=tf.placeholder(tf.float32)):\n",
        "    learning_rate = 1e-4\n",
        "    steps_number = 1000\n",
        "    batch_size = 100\n",
        "\n",
        "    # Read data\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "    # Define the loss function\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output))\n",
        "\n",
        "    # Training step\n",
        "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "    # Accuracy calculation\n",
        "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    # Run the training\n",
        "    sess = tf.InteractiveSession()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for i in range(steps_number):\n",
        "        # Get the next batch\n",
        "        input_batch, labels_batch = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # Print the accuracy progress on the batch every 100 steps\n",
        "        if i%100 == 0:\n",
        "            train_accuracy = accuracy.eval(feed_dict={training_data: input_batch, labels: labels_batch, keep_prob: 1.0})\n",
        "            print(\"Step %d, training batch accuracy %g %%\"%(i, train_accuracy*100))\n",
        "\n",
        "        # Run the training step\n",
        "        train_step.run(feed_dict={training_data: input_batch, labels: labels_batch, keep_prob: 0.5})\n",
        "\n",
        "    print(\"The end of training!\")\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    test_accuracy = accuracy.eval(feed_dict={training_data: mnist.test.images, labels: mnist.test.labels, keep_prob: 1.0})\n",
        "    print(\"Test accuracy: %g %%\"%(test_accuracy*100))\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "image_size = 28\n",
        "labels_size = 10\n",
        "hidden_size = 1024\n",
        "\n",
        "# Define placeholders\n",
        "training_data = tf.placeholder(tf.float32, [None, image_size*image_size])\n",
        "labels = tf.placeholder(tf.float32, [None, labels_size])\n",
        "\n",
        "# Variables for the hidden layer\n",
        "W_h = tf.Variable(tf.truncated_normal([image_size*image_size, hidden_size], stddev=0.1))\n",
        "b_h = tf.Variable(tf.constant(0.1, shape=[hidden_size]))\n",
        "\n",
        "# Hidden layer with reLU activation function\n",
        "hidden = tf.nn.relu(tf.matmul(training_data, W_h) + b_h)\n",
        "\n",
        "# Variables for the output layer\n",
        "W = tf.Variable(tf.truncated_normal([hidden_size, labels_size], stddev=0.1))\n",
        "b = tf.Variable(tf.constant(0.1, shape=[labels_size]))\n",
        "\n",
        "# Connect hidden to the output layer\n",
        "output = tf.matmul(hidden, W) + b\n",
        "\n",
        "train_network(training_data, labels, output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0806 09:44:06.313695 140110635681664 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, training batch accuracy 18 %\n",
            "Step 100, training batch accuracy 81 %\n",
            "Step 200, training batch accuracy 89 %\n",
            "Step 300, training batch accuracy 91 %\n",
            "Step 400, training batch accuracy 92 %\n",
            "Step 500, training batch accuracy 94 %\n",
            "Step 600, training batch accuracy 95 %\n",
            "Step 700, training batch accuracy 92 %\n",
            "Step 800, training batch accuracy 93 %\n",
            "Step 900, training batch accuracy 92 %\n",
            "The end of training!\n",
            "Test accuracy: 93.95 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_1ZZaROVW-l",
        "colab_type": "code",
        "outputId": "76a293aa-0048-4760-86bc-2ae131b94868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "#Convolutional layer\n",
        "import tensorflow as tf\n",
        "\n",
        "image_size = 28\n",
        "labels_size = 10\n",
        "hidden_size = 1024\n",
        "\n",
        "# Define placeholders\n",
        "training_data = tf.placeholder(tf.float32, [None, image_size*image_size])\n",
        "training_images = tf.reshape(training_data, [-1, image_size, image_size, 1])\n",
        "\n",
        "labels = tf.placeholder(tf.float32, [None, labels_size])\n",
        "\n",
        "# 1st convolutional layer variables\n",
        "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
        "b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
        "\n",
        "# 1st convolution & max pooling\n",
        "conv1 = tf.nn.relu(tf.nn.conv2d(training_images, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
        "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# 2nd convolutional layer variables\n",
        "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
        "b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "\n",
        "# 2nd convolution & max pooling\n",
        "conv2 = tf.nn.relu(tf.nn.conv2d(pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
        "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "# Flatten the 2nd convolution layer\n",
        "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
        "\n",
        "#Variables for the hidden dense layer\n",
        "W_h = tf.Variable(tf.truncated_normal([7 * 7 * 64, hidden_size], stddev=0.1))\n",
        "b_h = tf.Variable(tf.constant(0.1, shape=[hidden_size]))\n",
        "\n",
        "# Hidden layer with reLU activation function\n",
        "hidden = tf.nn.relu(tf.matmul(pool2_flat, W_h) + b_h)\n",
        "\n",
        "# Dropout\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "hidden_drop = tf.nn.dropout(hidden, keep_prob)\n",
        "\n",
        "# Variables to be tuned\n",
        "W = tf.Variable(tf.truncated_normal([hidden_size, labels_size], stddev=0.1))\n",
        "b = tf.Variable(tf.constant(0.1, shape=[labels_size]))\n",
        "\n",
        "# Connect hidden to the output layer\n",
        "output = tf.matmul(hidden_drop, W) + b\n",
        "\n",
        "train_network(training_data, labels, output, keep_prob)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0806 09:45:21.304094 140110635681664 deprecation.py:506] From <ipython-input-61-2062566804ad>:41: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 0, training batch accuracy 15 %\n",
            "Step 100, training batch accuracy 86 %\n",
            "Step 200, training batch accuracy 96 %\n",
            "Step 300, training batch accuracy 95 %\n",
            "Step 400, training batch accuracy 95 %\n",
            "Step 500, training batch accuracy 94 %\n",
            "Step 600, training batch accuracy 94 %\n",
            "Step 700, training batch accuracy 97 %\n",
            "Step 800, training batch accuracy 96 %\n",
            "Step 900, training batch accuracy 97 %\n",
            "The end of training!\n",
            "Test accuracy: 97.06 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhfAEIBaN9Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}